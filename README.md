# TRVCloud Intranet Demo App

## Installation and Setup

### Branches
This project includes two primary branches:

- **CF-Push-Ready branch**: Intended for deployment to **Tanzu Platform for Cloud Foundry**. Use this branch if you want to deploy the demo app to Cloud Foundry.  Requires you to pre-provision services (described below)
- **Main branch**: Primarily for local development. It requires **Ollama** running locally with an embeddings model (e.g., **nomic-embed-text**) and an LLM model (e.g., **gemma2:2b**), plus a Postgres database with the PGVector extension.

### Cloud Foundry (Tanzu Platform for Cloud Foundry) Setup

1. **Provision Services on Cloud Foundry**  
   Provision the following services:
   - **Postgres** service (with PGVector extension enabled by default)
   - **Embeddings Model** service (e.g., nomic-embed-text) via the **GenAI** tile
   - **LLM Model** service (e.g., gemma2:2b) via the **GenAI** tile

2. **Deploy to Cloud Foundry**  
   Switch to the **CF-Push-Ready** branch and push the app:
   ```bash
   git checkout CF-Push-Ready
   cf push

The app will automatically bind to the provisioned services. The code reads the required service credentials from `VCAP_SERVICES` using the `cfenv` utility, so the bindings are handled automatically.

### Local Development Setup

1. **Prerequisites**  
   To run the app locally, you will need:
   - **Ollama** running with the **nomic-embed-text** model (for embeddings) and **gemma2:2b** model (for LLM)
   - **Postgres** with the **PGVector** extension

2. **Clone the Repository and Install Dependencies**
   ```bash
   git clone https://github.com/yourusername/trvcloud-intranet.git
   cd trvcloud-intranet
   ```

3. **Set Up a Virtual Environment (optional but recommended)**
   ```bash
   python3 -m venv venv
   source venv/bin/activate   # On Windows, use `venv\Scripts\activate`
   pip install -r requirements.txt
   ```

4. **Set Up Environment Variables**  
   For local development, set the following variables (via a `.env` file or exporting manually):
   - For **Ollama** (local LLM and embeddings model):
     ```bash
     LLM_API_BASE=http://localhost:11434
     LLM_API_KEY=your_ollama_api_key
     EMBEDDING_API_BASE=http://localhost:11434
     EMBEDDING_API_KEY=your_ollama_api_key
     ```
   - For **Postgres** (local database with PGVector):
     ```bash
     DATABASE_URL=postgresql://postgres:postgres@localhost:5432/trvcloud
     ```

5. **Run the App Locally**
   ```bash
   python techbrese_intranet.py
   ```
   The app will be available at [http://127.0.0.1:5000](http://127.0.0.1:5000).

6. **Database Schema Initialization**

The app includes an **init_db.py** script to automatically initialize the database schema when running in Cloud Foundry.  When running the app locally you'll have to initialize it yoruself:
To initialize the database:
```bash
python init_db.py
```
This script creates the necessary schema so you do not need to run any manual SQL commands.

## How It Works

1. **Handbook Embedding and Update:**  
   - When the app starts or when you update the handbook via the `/update-handbook` route, the app reads the handbook content.
   - The content is split into chunks using the `chunk_text()` function.
   - Each chunk is passed to the `get_embedding()` function, which calls the embeddings API (using the GenAI tile service) to generate an embedding vector.
   - The `update_embeddings()` function then stores the chunk and its corresponding embedding in the Postgres database.

2. **Querying via Chatbot:**  
   - When you submit a query through the chatbot interface (`/api/chat` route), the query is first embedded using the `get_embedding()` function.
   - The app retrieves the most relevant handbook chunks from the database using the `retrieve_context()` function.
   - The retrieved context is combined with your query to form an augmented prompt, which is sent to the LLM service to generate a response.

3. **Service Bindings:**  
   - On Cloud Foundry, the app automatically reads service connection details from `VCAP_SERVICES` using the `cfenv` utility. This handles binding to the Postgres, LLM, and Embeddings services without additional configuration.  Note however, if you change the name of the model, you'll have to update the code.  This could be fixed if you want to rewrite the code so it parses the json to look for the "capabilities" key then extracts the credentials there.  

## Troubleshooting

- **Timeouts or Slow Responses:**  
  Adjust the timeout values in your API calls and Gunicorn settings (in your Procfile, e.g., `--timeout 60`) if you experience delays.  I have the LLM running on a non-GPU enabled VM and the responses are very slow.  
  
- **"Failed to Fetch" Errors:**  
  These often indicate network issues or timeouts. Check your service endpoints and logs for further details.

## Contributing

This repository is a demo app intended to showcase the GenAI capabilities of Tanzu Platform for Cloud Foundry. Contributions and improvements are welcome!  
If youâ€™d like to contribute:
1. Fork the repository.
2. Create a feature branch.
3. Make your changes and commit them.
4. Open a pull request.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
```
