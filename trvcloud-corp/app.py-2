import os
import json
import requests
import markdown
import psycopg2
from flask import Flask, render_template, request, redirect, url_for

app = Flask(__name__)

def get_vcap_services():
    """Parse VCAP_SERVICES environment variable into a Python dict."""
    vcap_services = os.getenv('VCAP_SERVICES')
    if vcap_services:
        return json.loads(vcap_services)
    return {}

def get_genai_service(capability):
    """
    Searches through the VCAP_SERVICES for a genai service with the given capability.
    Returns the service credentials if found, None otherwise.
    """
    vcap_services = get_vcap_services()
    genai_services = vcap_services.get('genai', [])
    
    for service in genai_services:
        creds = service.get('credentials', {})
        capabilities = creds.get('model_capabilities', [])
        if capability in capabilities:
            return creds
    return None

# ------------------------------------------------------------------------------
# Configure LLM (chat) endpoint using CF service
chat_service = get_genai_service("chat")
if chat_service:
    LLM_API_BASE = chat_service.get("api_base")
    LLM_API_KEY = chat_service.get("api_key")
    LLM_MODEL = chat_service.get("model_name", "gemma2:2b")
    print("Using CF LLM endpoint:", LLM_API_BASE)
else:
    LLM_API_BASE = os.environ.get("LLM_GENERATE_ENDPOINT", "http://localhost:11434/api/generate")
    LLM_API_KEY = os.environ.get("LLM_API_KEY", "")
    LLM_MODEL = "gemma2:2b"
    print("Using local LLM endpoint:", LLM_API_BASE)

# ------------------------------------------------------------------------------
# Configure Embedding endpoint using CF service
embedding_service = get_genai_service("embedding")
if embedding_service:
    EMBEDDING_API_BASE = embedding_service.get("api_base")
    EMBEDDING_API_KEY = embedding_service.get("api_key")
    EMBEDDING_MODEL = embedding_service.get("model_name", "nomic-embed-text")
    print("Using CF Embedding endpoint:", EMBEDDING_API_BASE)
else:
    EMBEDDING_API_BASE = os.environ.get("EMBEDDING_ENDPOINT", "http://localhost:11434/api/embeddings")
    EMBEDDING_API_KEY = os.environ.get("EMBEDDING_API_KEY", "")
    EMBEDDING_MODEL = "nomic-embed-text"
    print("Using local Embedding endpoint:", EMBEDDING_API_BASE)

# ------------------------------------------------------------------------------
# Configure Postgres connection using CF service
def get_postgres_credentials():
    """Get Postgres credentials from VCAP_SERVICES."""
    vcap_services = get_vcap_services()
    postgres_services = vcap_services.get('postgres', [])
    
    if postgres_services:
        return postgres_services[0].get('credentials', {})
    return None

postgres_creds = get_postgres_credentials()
if postgres_creds:
    DB_URL = postgres_creds.get('uri')
    print("Using CF Postgres URI:", DB_URL)
else:
    DB_URL = os.environ.get("DATABASE_URL", "postgresql://postgres:postgres@127.0.0.1:5432/postgres-db")
    print("Using local Postgres URI:", DB_URL)

# ------------------------------------------------------------------------------
# Other configuration variables
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
HANDBOOK_FILE = os.path.join(BASE_DIR, "handbook.md")

# ------------------------------------------------------------------------------
# Helper Functions for Embeddings and Retrieval

def get_embedding(text):
    """
    Gets embeddings using OpenAI-compatible API format
    """
    payload = {
        "input": text,
        "model": EMBEDDING_MODEL
    }
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {EMBEDDING_API_KEY}"
    }
    
    response = requests.post(
        f"{EMBEDDING_API_BASE}/embeddings",
        json=payload,
        headers=headers
    )
    
    if response.status_code == 200:
        data = response.json()
        return data["data"][0]["embedding"]
    else:
        raise Exception(f"Embedding API error: {response.status_code} {response.text}")

def update_embeddings(text):
    """
    Splits the handbook text into chunks, computes an embedding for each chunk,
    and stores the result in the handbook_chunks table. Existing rows are cleared.
    """
    chunks = chunk_text(text)  # Use your existing chunking function.
    print(f"update_embeddings: Processing {len(chunks)} chunks.")
    
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    
    # Clear existing data
    cur.execute("DELETE FROM handbook_chunks;")
    conn.commit()
    
    for chunk in chunks:
        try:
            embedding = get_embedding(chunk)
        except Exception as e:
            print("Error getting embedding for a chunk:", e)
            continue
        
        # Insert the chunk and its embedding. Make sure the embedding format
        # matches your database column (you might need to convert it to a string
        # or a list depending on your PGVector setup).
        cur.execute(
            "INSERT INTO handbook_chunks (chunk_text, embedding) VALUES (%s, %s);",
            (chunk, embedding)
        )
        conn.commit()
    
    cur.close()
    conn.close()
    print("update_embeddings: Done updating embeddings.")


def chunk_text(text, chunk_size=500):
    """
    Splits the text into chunks based on double newlines, and further splits
    long paragraphs if they exceed chunk_size.
    """
    paragraphs = text.split('\n\n')
    chunks = []
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        if len(para) > chunk_size:
            words = para.split()
            current_chunk = ""
            for word in words:
                if len(current_chunk) + len(word) + 1 > chunk_size:
                    chunks.append(current_chunk.strip())
                    current_chunk = word + " "
                else:
                    current_chunk += word + " "
            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            chunks.append(para)
    print(f"chunk_text: Found {len(chunks)} chunks.")
    return chunks

def retrieve_context(query, top_n=5):
    """
    Computes the query embedding and retrieves the top_n most similar handbook chunks
    from the database using PGVector similarity search.
    """
    query_embedding = get_embedding(query)
    
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    
    cur.execute("""
        SELECT chunk_text 
        FROM handbook_chunks 
        ORDER BY embedding <-> (%s)::vector
        LIMIT %s;
    """, (query_embedding, top_n))
    
    rows = cur.fetchall()
    cur.close()
    conn.close()
    
    context = "\n\n".join(row[0] for row in rows)
    print(f"retrieve_context: Retrieved {len(rows)} chunks.")
    return context

# ------------------------------------------------------------------------------
# Routes

@app.route('/')
def landing():
    return render_template('landing.html')

@app.route('/handbook')
def handbook():
    with open(HANDBOOK_FILE, 'r', encoding='utf-8') as file:
        md_content = file.read()
    handbook_html = markdown.markdown(md_content)
    return render_template('handbook.html', handbook=handbook_html)

@app.route('/edit-handbook', methods=['GET'])
def edit_handbook():
    with open(HANDBOOK_FILE, 'r', encoding='utf-8') as file:
        content = file.read()
    return render_template('edit_handbook.html', content=content)

@app.route('/update-handbook', methods=['POST'])
def update_handbook():
    new_content = request.form.get('handbook_content')
    print("Update-handbook route triggered; new content length:", len(new_content))
    with open(HANDBOOK_FILE, 'w', encoding='utf-8') as file:
        file.write(new_content)
    
    try:
        update_embeddings(new_content)  # Now actually process and store embeddings.
    except Exception as e:
        print("Error updating embeddings:", e)
    
    return redirect(url_for('handbook'))


@app.route('/api/chat', methods=['POST'])
def api_chat():
    data = request.get_json()
    user_message = data.get("message")
    if not user_message:
        return {"error": "No message provided"}, 400

    context = retrieve_context(user_message, top_n=5)
    print("Retrieved context:", context)
    
    payload = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": f"You are a helpful assistant. Use the following context to answer the user's question:\n\n{context}"},
            {"role": "user", "content": user_message}
        ]
    }
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LLM_API_KEY}"
    }
    
    print("Sending payload to LLM:", payload)
    response = requests.post(
        f"{LLM_API_BASE}/chat/completions",
        json=payload,
        headers=headers
    )
    print("LLM response:", response.status_code, response.text)
    
    if response.status_code == 200:
        response_data = response.json()
        return {
            "response": response_data["choices"][0]["message"]["content"]
        }, 200
    else:
        return {"error": "Failed to get response from LLM", "details": response.text}, 500

@app.route('/chatbot')
def chatbot():
    return render_template('chatbot.html')

if __name__ == '__main__':
    app.run(debug=True)